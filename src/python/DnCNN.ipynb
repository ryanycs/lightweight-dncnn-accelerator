{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DnCNN_PyTorch.models import DnCNN\n",
    "channels = 1\n",
    "num_of_layers = 5\n",
    "input_size = (channels, 256, 256)\n",
    "model = DnCNN(channels=channels, num_of_layers=num_of_layers, do_fuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.ao.quantization as tq\n",
    "\n",
    "\n",
    "class PowerOfTwoObserver(tq.MinMaxObserver):\n",
    "    \"\"\"\n",
    "    Observer module for power-of-two quantization (dyadic quantization with b = 1).\n",
    "    \"\"\"\n",
    "    def scale_approximate(self, scale: float, max_shift_amount=8) -> float:\n",
    "        # Finding the nearest power of two by converting the scale to its binary representation\n",
    "        scale_log2 = torch.ceil(torch.log2(torch.tensor(scale)))\n",
    "        scale_log2 = torch.clamp(scale_log2, max=max_shift_amount)\n",
    "        power_of_two_scale = 2 ** scale_log2\n",
    "\n",
    "        return power_of_two_scale\n",
    "\n",
    "    def calculate_qparams(self):\n",
    "        \"\"\"Calculates the quantization parameters with scale as power of two.\"\"\"\n",
    "        min_val, max_val = self.min_val.item(), self.max_val.item()\n",
    "\n",
    "        \"\"\" Calculate zero_point as in the base class \"\"\"\n",
    "        # Compute scale\n",
    "        scale = max(abs(min_val), abs(max_val)) / (2**7 - 1)  # For 8-bit symmetric quantization\n",
    "\n",
    "        if(self.dtype == torch.qint8):\n",
    "          zero_point = 0\n",
    "        else :\n",
    "          zero_point = 128\n",
    "\n",
    "        scale = self.scale_approximate(scale)\n",
    "        scale = torch.tensor(scale, dtype=torch.float32)\n",
    "        zero_point = torch.tensor(zero_point, dtype=torch.int64)\n",
    "\n",
    "        return scale, zero_point\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"min_val={self.min_val}, max_val={self.max_val}, scale=PowerOfTwo\"\n",
    "    \n",
    "class CustomQConfig(Enum):\n",
    "    POWER2 = tq.QConfig(\n",
    "        activation=PowerOfTwoObserver.with_args(\n",
    "            dtype=torch.quint8, qscheme=torch.per_tensor_symmetric\n",
    "        ),\n",
    "        weight=PowerOfTwoObserver.with_args(\n",
    "            dtype=torch.qint8, qscheme=torch.per_tensor_symmetric\n",
    "        ),\n",
    "    )\n",
    "    DEFAULT = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "\n",
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None, noise_std=25.0):\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.png\")))\n",
    "        self.transform = transform\n",
    "        self.noise_std = noise_std / 255.0\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"L\")\n",
    "        if self.transform:\n",
    "            clean_tensor = self.transform(img)\n",
    "        else:\n",
    "            clean_tensor = transforms.ToTensor()(img)\n",
    "        noise = torch.FloatTensor(clean_tensor.size()).normal_(mean=0, std=self.noise_std)\n",
    "        noisy_tensor = clean_tensor + noise\n",
    "        return noisy_tensor\n",
    "\n",
    "def get_calibration_loader(image_dir=\"./data/train\", image_size=40, batch_size=1, noise_std=25.0):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    dataset = ImageFolderDataset(image_dir, transform=transform, noise_std=noise_std)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def preprocess_filename(filename: str, existed: str = \"keep_both\") -> str:\n",
    "    if existed == \"overwrite\":\n",
    "        pass\n",
    "    elif existed == \"keep_both\":\n",
    "        base, ext = os.path.splitext(filename)\n",
    "        cnt = 1\n",
    "        while os.path.exists(filename):\n",
    "            filename = f\"{base}-{cnt}{ext}\"\n",
    "            cnt += 1\n",
    "    elif existed == \"raise\" and os.path.exists(filename):\n",
    "        raise FileExistsError(f\"{filename} already exists.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown value for 'existed': {existed}\")\n",
    "    return filename\n",
    "\n",
    "def save_model(\n",
    "    model, filename: str, verbose: bool = True, existed: str = \"keep_both\"\n",
    ") -> None:\n",
    "    filename = preprocess_filename(filename, existed)\n",
    "\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    if verbose:\n",
    "        print(f\"Model saved at {filename} ({os.path.getsize(filename) / 1e6} MB)\")\n",
    "    else:\n",
    "        print(f\"Model saved at {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.ao.quantization as tq\n",
    "from DnCNN_PyTorch.models import DnCNN\n",
    "\n",
    "\"\"\" Calibrate Method \"\"\"\n",
    "def calibrate(model, loader, device=\"cpu\"):\n",
    "    model.eval().to(device)   \n",
    "    for x in loader:       \n",
    "        model(x.to(device))  \n",
    "        break     \n",
    "\n",
    "\"\"\" Load Pretrained Model \"\"\"\n",
    "model_path = \"./model/DnCNN_layer5.pt\"\n",
    "channels = 1\n",
    "num_of_layers = 5\n",
    "\n",
    "Pretrained_model = DnCNN(channels=channels, num_of_layers=num_of_layers)\n",
    "Pretrained_model.eval() \n",
    "Pretrained_model.cpu()\n",
    "\n",
    "state_dict = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k.replace('module.', '')\n",
    "    new_state_dict[name] = v\n",
    "Pretrained_model.load_state_dict(new_state_dict)\n",
    "\n",
    "\"\"\" Fuse Modules \"\"\"\n",
    "Pretrained_model.fuse_layers()\n",
    "\n",
    "\"\"\" Configure Quantization \"\"\"\n",
    "fused_model = tq.QuantWrapper(Pretrained_model)\n",
    "fused_model.qconfig = CustomQConfig.POWER2.value \n",
    "print(f\"Quantization backend: {fused_model.qconfig}\")\n",
    "\n",
    "\"\"\" Apply Quantization Preparation \"\"\"\n",
    "tq.prepare(fused_model, inplace=True)\n",
    "\n",
    "\"\"\" Calibration \"\"\"\n",
    "calibrate(fused_model, get_calibration_loader(image_dir=\"./DnCNN_PyTorch/data/train\"))\n",
    "\n",
    "\"\"\" Convert Model to Quantized Version \"\"\"\n",
    "tq.convert(fused_model.cpu(), inplace=True)\n",
    "\n",
    "\"\"\" Save Quantized Model \"\"\"\n",
    "quantized_model_path = \"./DnCNN_5_layers_int8.pt\"\n",
    "save_model(fused_model, quantized_model_path, existed=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.ao.quantization as tq\n",
    "from DnCNN_PyTorch.models import DnCNN\n",
    "\n",
    "channels = 1\n",
    "num_of_layers = 5\n",
    "input_size = (channels, 256, 256)\n",
    "backend = \"power2\"\n",
    "\n",
    "quantized_model = DnCNN(channels=channels, num_of_layers=num_of_layers)\n",
    "quantized_model.eval()\n",
    "quantized_model.cpu()\n",
    "quantized_model.fuse_layers()\n",
    "quantized_model = tq.QuantWrapper(quantized_model)\n",
    "qconfig = CustomQConfig[\"POWER2\"].value\n",
    "quantized_model.qconfig = qconfig\n",
    "tq.prepare(quantized_model, inplace=True)\n",
    "tq.convert(quantized_model, inplace=True)\n",
    "\n",
    "quantized_model_path = \"./model/DnCNN_5_layers_int8.pt\"\n",
    "\n",
    "quantized_model.load_state_dict(torch.load(quantized_model_path, map_location=\"cpu\"))\n",
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from DnCNN_PyTorch.utils import batch_PSNR\n",
    "\n",
    "def normalize(data):\n",
    "    return data / 255.0\n",
    "device = torch.device('cpu')\n",
    "quantized_model.to(device).eval()\n",
    "\n",
    "test_data = \"Set12\"\n",
    "input_dir  = os.path.join(\"DnCNN_PyTorch\", \"data\", test_data) \n",
    "num_layers = \"layer5\"     \n",
    "    \n",
    "output_dir = os.path.join(\"results\",num_layers ,test_data)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print('Loading test images...\\n')\n",
    "files = sorted(glob.glob(os.path.join(input_dir, '*.png')))\n",
    "\n",
    "psnr_sum = 0.0\n",
    "test_noiseL = 25.0\n",
    "\n",
    "for f in files:\n",
    "    name = os.path.basename(f)\n",
    "    gray = cv2.imread(f, cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "    img  = normalize(gray)\n",
    "    ISource = torch.from_numpy(img).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    torch.manual_seed(0)\n",
    "    noise = torch.randn_like(ISource) * (test_noiseL / 255.0)\n",
    "    INoisy = ISource + noise\n",
    "    with torch.no_grad():\n",
    "        denoised = quantized_model(INoisy)\n",
    "        Out = torch.clamp(INoisy - denoised, 0.0, 1.0)\n",
    "    psnr = batch_PSNR(Out, ISource, data_range=1.0)\n",
    "    psnr_sum += psnr\n",
    "    print(f\"{name}  PSNR: {psnr:.4f}\")\n",
    "\n",
    "avg_psnr = psnr_sum / len(files)\n",
    "print(f\"\\nAverage PSNR on test data: {avg_psnr:.4f}\")\n",
    "print(f\"Denoised images saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.ao.quantization as tq\n",
    "from DnCNN_PyTorch.models import DnCNN\n",
    "\n",
    "model_path = \"./model/DnCNN_5_layers.pt\"\n",
    "channels = 1\n",
    "num_of_layers = 5\n",
    "\n",
    "model = DnCNN(channels=channels, num_of_layers=num_of_layers)\n",
    "model.eval() \n",
    "model.cpu()\n",
    "state_dict = torch.load(model_path, map_location='cpu')\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k.replace('module.', '') \n",
    "    new_state_dict[name] = v\n",
    "model.load_state_dict(new_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from DnCNN_PyTorch.utils import batch_PSNR\n",
    "\n",
    "def normalize(data):\n",
    "    return data / 255.0\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model.to(device).eval()\n",
    "\n",
    "test_data = \"Set12\"\n",
    "input_dir  = os.path.join(\"DnCNN_PyTorch\", \"data\", test_data)\n",
    "output_dir = os.path.join(\"results\", test_data)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print('Loading test images...\\n')\n",
    "files = sorted(glob.glob(os.path.join(input_dir, '*.png')))\n",
    "\n",
    "psnr_sum = 0.0\n",
    "test_noiseL = 25.0\n",
    "\n",
    "for f in files:\n",
    "    name = os.path.basename(f)\n",
    "    gray = cv2.imread(f, cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "    img  = normalize(gray)\n",
    "    ISource = torch.from_numpy(img).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    torch.manual_seed(0)\n",
    "    noise = torch.randn_like(ISource) * (test_noiseL / 255.0)\n",
    "    INoisy = ISource + noise\n",
    "    with torch.no_grad():\n",
    "        denoised = model(INoisy)\n",
    "        Out = torch.clamp(INoisy - denoised, 0.0, 1.0)\n",
    "    psnr = batch_PSNR(Out, ISource, data_range=1.0)\n",
    "    psnr_sum += psnr\n",
    "    print(f\"{name}  PSNR: {psnr:.4f}\")\n",
    "\n",
    "avg_psnr = psnr_sum / len(files)\n",
    "print(f\"\\nAverage PSNR on test data: {avg_psnr:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CVDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
