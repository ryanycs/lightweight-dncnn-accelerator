{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "parent_dir = os.path.abspath(\"..\")\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from collections import OrderedDict\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.ao.quantization as tq\n",
    "from config import Config\n",
    "from model import DnCNN\n",
    "from PIL import Image\n",
    "from utils import batch_PSNR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerOfTwoObserver(tq.MinMaxObserver):\n",
    "    \"\"\"\n",
    "    Observer module for power-of-two quantization (dyadic quantization with b = 1).\n",
    "    \"\"\"\n",
    "\n",
    "    def scale_approximate(self, scale: float, max_shift_amount=8) -> float:\n",
    "        # Finding the nearest power of two by converting the scale to its binary representation\n",
    "        scale_log2 = torch.ceil(torch.log2(torch.tensor(scale)))\n",
    "        scale_log2 = torch.clamp(scale_log2, max=max_shift_amount)\n",
    "        power_of_two_scale = 2**scale_log2\n",
    "\n",
    "        return power_of_two_scale\n",
    "\n",
    "    def calculate_qparams(self):\n",
    "        \"\"\"\n",
    "        Calculates the quantization parameters with scale as power of two.\n",
    "        \"\"\"\n",
    "        min_val, max_val = self.min_val.item(), self.max_val.item()\n",
    "\n",
    "        \"\"\" Calculate zero_point as in the base class \"\"\"\n",
    "        # Compute scale\n",
    "        scale = max(abs(min_val), abs(max_val)) / (\n",
    "            2**7 - 1\n",
    "        )  # For 8-bit symmetric quantization\n",
    "\n",
    "        if self.dtype == torch.qint8:\n",
    "            zero_point = 0\n",
    "        else:\n",
    "            zero_point = 128\n",
    "\n",
    "        scale = self.scale_approximate(scale)\n",
    "        zero_point = torch.tensor(zero_point, dtype=torch.int64)\n",
    "\n",
    "        return scale, zero_point\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"min_val={self.min_val}, max_val={self.max_val}, scale=PowerOfTwo\"\n",
    "\n",
    "\n",
    "class CustomQConfig(Enum):\n",
    "    POWER2 = tq.QConfig(\n",
    "        activation=PowerOfTwoObserver.with_args(\n",
    "            dtype=torch.quint8, qscheme=torch.per_tensor_symmetric\n",
    "        ),\n",
    "        weight=PowerOfTwoObserver.with_args(\n",
    "            dtype=torch.qint8, qscheme=torch.per_tensor_symmetric\n",
    "        ),\n",
    "    )\n",
    "    DEFAULT = None\n",
    "\n",
    "\n",
    "def normalize(data):\n",
    "    return data / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Quantization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 1\n",
    "num_of_layers = 5\n",
    "input_size = (channels, 256, 256)\n",
    "backend = \"power2\"\n",
    "\n",
    "quantized_model = DnCNN(channels=channels, num_of_layers=num_of_layers)\n",
    "quantized_model.eval()\n",
    "quantized_model.cpu()\n",
    "quantized_model.fuse_layers()\n",
    "quantized_model = tq.QuantWrapper(quantized_model)\n",
    "qconfig = CustomQConfig[\"POWER2\"].value\n",
    "quantized_model.qconfig = qconfig\n",
    "tq.prepare(quantized_model, inplace=True)\n",
    "tq.convert(quantized_model, inplace=True)\n",
    "\n",
    "quantized_model.load_state_dict(torch.load(Config.quantized_model_path, map_location=\"cpu\"))\n",
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "quantized_model.to(device).eval()\n",
    "\n",
    "test_data = \"Set12\"\n",
    "input_dir = os.path.join(Config.base_dir, \"data\", test_data)\n",
    "num_layers = \"layer5\"\n",
    "\n",
    "output_dir = os.path.join(\"output\", \"int8\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Loading test images...\\n\")\n",
    "files = sorted(glob.glob(os.path.join(input_dir, \"*.png\")))\n",
    "\n",
    "psnr_sum = 0.0\n",
    "test_noiseL = 25.0\n",
    "\n",
    "for f in files:\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    name = os.path.basename(f)\n",
    "    gray = Image.open(f).convert(\"L\")\n",
    "    img = normalize(np.array(gray, dtype=np.float32))\n",
    "    ISource = torch.from_numpy(img).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    noise = torch.randn_like(ISource) * (test_noiseL / 255.0)\n",
    "    INoisy = ISource + noise\n",
    "\n",
    "    with torch.no_grad():\n",
    "        denoised = quantized_model(INoisy)\n",
    "        out = torch.clamp(INoisy - denoised, 0.0, 1.0)\n",
    "\n",
    "    psnr = batch_PSNR(out, ISource, data_range=1.0)\n",
    "    psnr_sum += psnr\n",
    "    print(f\"{name}  PSNR: {psnr:.4f}\")\n",
    "\n",
    "    # save denoised image\n",
    "    out = out.squeeze().cpu().numpy() * 255.0\n",
    "    out = np.clip(out, 0, 255).astype(np.uint8)\n",
    "    output_path = os.path.join(output_dir, name)\n",
    "    Image.fromarray(out).save(output_path)\n",
    "\n",
    "avg_psnr = psnr_sum / len(files)\n",
    "print(f\"\\nAverage PSNR on test data: {avg_psnr:.4f}\")\n",
    "print(f\"Denoised images saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Full Precision Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 1\n",
    "num_of_layers = 5\n",
    "\n",
    "model = DnCNN(channels=channels, num_of_layers=num_of_layers)\n",
    "model.eval()\n",
    "model.cpu()\n",
    "state_dict = torch.load(Config.model_path, map_location=\"cpu\")\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k.replace(\"module.\", \"\")\n",
    "    new_state_dict[name] = v\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "test_data = \"Set12\"\n",
    "input_dir = os.path.join(Config.base_dir, \"data\", test_data)\n",
    "output_dir = os.path.join(\"output\", \"float32\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Loading test images...\\n\")\n",
    "files = sorted(glob.glob(os.path.join(input_dir, \"*.png\")))\n",
    "\n",
    "psnr_sum = 0.0\n",
    "test_noiseL = 25.0\n",
    "\n",
    "for f in files:\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    name = os.path.basename(f)\n",
    "    gray = Image.open(f).convert(\"L\")\n",
    "    img = normalize(np.array(gray, dtype=np.float32))\n",
    "    ISource = torch.from_numpy(img).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    noise = torch.randn_like(ISource) * (test_noiseL / 255.0)\n",
    "    INoisy = ISource + noise\n",
    "\n",
    "    with torch.no_grad():\n",
    "        denoised = model(INoisy)\n",
    "        out = torch.clamp(INoisy - denoised, 0.0, 1.0)\n",
    "\n",
    "    psnr = batch_PSNR(out, ISource, data_range=1.0)\n",
    "    psnr_sum += psnr\n",
    "    print(f\"{name}  PSNR: {psnr:.4f}\")\n",
    "\n",
    "    # save denoised image\n",
    "    out = out.squeeze().cpu().numpy() * 255.0\n",
    "    out = np.clip(out, 0, 255).astype(np.uint8)\n",
    "    output_path = os.path.join(output_dir, name)\n",
    "    Image.fromarray(out).save(output_path)\n",
    "\n",
    "avg_psnr = psnr_sum / len(files)\n",
    "print(f\"\\nAverage PSNR on test data: {avg_psnr:.4f}\")\n",
    "print(f\"Denoised images saved to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
